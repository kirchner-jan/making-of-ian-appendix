{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for massaging different sources of text into a unified set of small .txt documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roam Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/21028979/recursive-iteration-through-nested-json-for-specific-key-in-python\n",
    "def item_generator(json_input, lookup_key):\n",
    "     if isinstance(json_input, dict):\n",
    "         for k, v in json_input.items():\n",
    "             if k == lookup_key:\n",
    "                 yield v\n",
    "             else:\n",
    "                 yield from  item_generator(v, lookup_key)\n",
    "     elif isinstance(json_input, list):\n",
    "         for item in json_input:\n",
    "             yield from item_generator(item, lookup_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roam Research\n",
    "import json , re\n",
    "with open('YOUR_ROAM_GRAPH.json') as json_file:\n",
    "     data = json.load(json_file)\n",
    "\n",
    "file2 = open(\"roam_research.txt\" , \"a\")\n",
    "for st in item_generator(data , \"string\"):\n",
    "    stripped_st = st.replace('{{[[r/moved]]}}',''); # repeat these two steps with all the formatting you want to remove\n",
    "    stripped_st = re.sub(r'\\(\\(.*\\)\\)' , '' , stripped_st);   \n",
    "    if len(stripped_st) > 0: # if there's anything left afterwards, add it to the output\n",
    "        stripped_st = stripped_st + '\\n';\n",
    "        file2.write(stripped_st)\n",
    "        \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into chunks for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!mkdir roam_files\n",
    "os.system('split -l 100 roam_research.txt roam_files/splitroam');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcrawling\n",
    "Do some webcrawling to resolve links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install trafilatura\n",
    "!pip3 install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def download_file(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url)    \n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura , re , os\n",
    "from tika import parser\n",
    "\n",
    "with open('roam_research.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "href_regex = r'https?://\\S+\\)' # find URLs\n",
    "urls = re.findall(href_regex, text)\n",
    "urls = [url[:-1] for url in urls]\n",
    "for url in urls:\n",
    "    fName = \"webcrawl/\" + url.replace('https','').replace('http','').replace('://','').replace('/','-')[:30] + \".txt\"\n",
    "    if not os.path.exists(fName):\n",
    "        try:\n",
    "            tFile = open(fName , \"a\")\n",
    "            if \".pdf\" not in url:\n",
    "                downloaded = trafilatura.fetch_url(url)\n",
    "                result = trafilatura.extract(downloaded, include_comments=False, include_tables=False, no_fallback=True)\n",
    "            else:\n",
    "                download_file(url, \"sample\")\n",
    "                raw = parser.from_file('sample.pdf')\n",
    "                result = raw['content']\n",
    "            if not result is None:\n",
    "                tFile.write(result)\n",
    "            tFile.close()\n",
    "        except:\n",
    "            print('oops') # master-level error handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty lines\n",
    "import os\n",
    "fList = os.listdir('webcrawl')\n",
    "cc = 0\n",
    "for file in fList:\n",
    "    print(cc)\n",
    "    cc = cc + 1\n",
    "    os.system(\"awk ' /^$/ { print; } /./ { printf(\\\"%s \\\", $0); } ' webcrawl/\" + file + \" > webcrawl/tmp.txt\")\n",
    "    os.system(\"mv webcrawl/tmp.txt webcrawl/\" + file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split files\n",
    "fList = os.listdir('webcrawl/')\n",
    "os.system('mkdir webcrawl/splitcrawl')\n",
    "for file in fList:\n",
    "    os.system('split -l 100 webcrawl/' + file +' webcrawl/splitcrawl/split'+ file[:-4] + '.txt');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhatsApp\n",
    "Whatsapp already comes separated by user - but still worth it to chunk it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fList = os.listdir('whatsapp/')\n",
    "os.system('mkdir whatsapp/splitwhatsapp')\n",
    "for file in fList:\n",
    "    os.system('mv whatsapp/' + file.replace(' ','\\ ') + ' whatsapp/' + file.replace(' ',''))\n",
    "    # The name that people have on Whatsapp on my phone is often different from the one in Roam/Email.\n",
    "    # It's a good idea to make these the same so that the language model can identify them.\n",
    "    os.system(\"sed -i '' 's/WHATSAPP_NAME/REAL_NAME/g' whatsapp/\" + file) # repeat as often as necessary\n",
    "    os.system('split -l 400 whatsapp/' + file +' whatsapp/splitwhatsapp/split'+ file[:-4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook\n",
    "Facebook data just need ~~light~~ massive massaging and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"sed 's/{//g' facebook.txt > facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' 's/}//g' facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' 's/\\\"//g' facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' 's/  name: //g' facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' 's/  message: //g' facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' '/^$/d' facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' 's/,$/: /g' facebook_stripped.txt\")\n",
    "os.system(\"cat facebook_stripped.txt |  paste -d ' ' - - | tee facebook_stripped_full.txt\")\n",
    "os.system(\"mv facebook_stripped_full.txt facebook_stripped.txt\")\n",
    "os.system(\"sed -i '' 's/  / /g' facebook_stripped.txt\")\n",
    "# here again you can replace Facebook names with real names\n",
    "os.system(\"sed -i '' 's/FACEBOOK_NAME/REAL_NAME/g' facebook_stripped.txt\")\n",
    "os.system(\"mkdir facebook\")\n",
    "os.system('split -l 100 facebook_stripped.txt facebook/splitfacebook');\n",
    "os.system(\"rm facebook_stripped.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email\n",
    "email also needs a lot of massaging after extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailbox\n",
    "mbox_obj = mailbox.mbox('Sent.mbox', create=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7166922/extracting-the-body-of-an-email-from-mbox-file-decoding-it-to-plain-text-regard\n",
    "def getcharsets(msg):\n",
    "    charsets = set({})\n",
    "    for c in msg.get_charsets():\n",
    "        if c is not None:\n",
    "            charsets.update([c])\n",
    "    return charsets\n",
    "\n",
    "def handleerror(errmsg, emailmsg,cs):\n",
    "    print('oops') # yes, I feel bad about this too. \n",
    "\n",
    "def getbodyfromemail(msg):\n",
    "    body = None\n",
    "    #Walk through the parts of the email to find the text body.    \n",
    "    if msg.is_multipart():    \n",
    "        for part in msg.walk():\n",
    "            # If part is multipart, walk through the subparts.            \n",
    "            if part.is_multipart(): \n",
    "                for subpart in part.walk():\n",
    "                    if subpart.get_content_type() == 'text/plain':\n",
    "                        # Get the subpart payload (i.e the message body)\n",
    "                        body = subpart.get_payload(decode=True) \n",
    "                        #charset = subpart.get_charset()\n",
    "            # Part isn't multipart so get the email body\n",
    "            elif part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)\n",
    "                #charset = part.get_charset()\n",
    "\n",
    "    # If this isn't a multi-part message then get the payload (i.e the message body)\n",
    "    elif msg.get_content_type() == 'text/plain':\n",
    "        body = msg.get_payload(decode=True) \n",
    "\n",
    "   # No checking done to match the charset with the correct part. \n",
    "    for charset in getcharsets(msg):\n",
    "        try:\n",
    "            body = body.decode(charset)\n",
    "        except UnicodeDecodeError:\n",
    "            handleerror(\"UnicodeDecodeError: encountered.\",msg,charset)\n",
    "        except AttributeError:\n",
    "             handleerror(\"AttributeError: encountered\" ,msg,charset)\n",
    "    return body    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for thisemail in mbox_obj:\n",
    "    body = getbodyfromemail(thisemail)\n",
    "    if thisemail['To'] is None:\n",
    "        thisemail['To'] = 'many'\n",
    "    filename = 'email/' + thisemail['Date'] +  thisemail['To'] \n",
    "    filename = filename[:50] + '.txt'\n",
    "    if not body is None:\n",
    "        stripped_body = re.sub(r'From:.*' , '' , body); # remove all the header of the email - boring & repetitive!\n",
    "        stripped_body = re.sub(r'Sent:.*' , '' , stripped_body);\n",
    "        stripped_body = re.sub(r'To:.*' , '' , stripped_body);\n",
    "        stripped_body = re.sub(r'Subject.*' , '' , stripped_body);\n",
    "        stripped_body = re.sub(r'Von:.*' , '' , body); # and then the same thing in German\n",
    "        stripped_body = re.sub(r'Gesendet:.*' , '' , body);\n",
    "        stripped_body = re.sub(r'An:.*' , '' , body);\n",
    "        stripped_body = re.sub(r'Betreff:.*' , '' , body);\n",
    "        stripped_body = re.sub(r'<.*>' , '' , stripped_body);\n",
    "        stripped_body = re.sub(r'>' , '' , stripped_body);\n",
    "        stripped_body = re.sub(r'Cc:.*' , '' , stripped_body);\n",
    "        stripped_body = re.sub(r'________________________________' , '' , stripped_body);\n",
    "        stripped_body = os.linesep.join([s for s in stripped_body.splitlines() if s])\n",
    "        try:\n",
    "            with open(filename.replace(' ' , ''), 'w') as f:\n",
    "                f.write(stripped_body)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telegram\n",
    "Telegram needs to be queried over the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install telethon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your API Key here: https://core.telegram.org/api/obtaining_api_id\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.types import PeerUser, PeerChat, PeerChannel\n",
    "import os\n",
    "client = TelegramClient('API_NAME', API_ID, 'API_KEY')\n",
    "await client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for dialog in client.iter_dialogs():\n",
    "    print(dialog.name, 'has ID', dialog.id)\n",
    "    if not os.path.isfile('telegram/' + dialog.name.replace('/','') + '.txt' ):\n",
    "        f = open('telegram/' + dialog.name.replace('/','') + '.txt' , 'w+');\n",
    "        async for message in client.iter_messages(dialog.id):\n",
    "            try:\n",
    "              if not (message.from_id is None):\n",
    "                  my_user    = await client.get_entity(message.from_id)\n",
    "                  f.write(my_user.first_name +  ' : ' + message.text)\n",
    "            except:\n",
    "                continue\n",
    "        f.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fList = os.listdir('telegram/')\n",
    "os.system('mkdir telegram/splittelegram')\n",
    "for file in fList:\n",
    "    os.system('mv telegram/' + file.replace(' ','\\ ') + ' telegram/' + file.replace(' ',''))\n",
    "    # again do that thing with the names\n",
    "    os.system(\"sed -i '' 's/TELEGRAM_NAME/REAL_NAME/g' telegram/\" + file.replace(' ',''))\n",
    "    os.system('split -l 100 telegram/' + file.replace(' ','') +' telegram/splittelegram/split'+ file[:-4].replace(' ',''));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epubs\n",
    "convert epubs to txt and split them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fList = os.listdir('epubs')\n",
    "for file in fList:\n",
    "    os.system('pandoc epubs/' + file.replace(' ' , '\\ ') + ' -t plain -o epubs_txt/' + file[:-4].replace(' ' , '\\ ') + 'txt')\n",
    "    os.system(\"awk ' /^$/ { print; } /./ { printf(\\\"%s \\\", $0); } ' epubs_txt/\" + file[:-4].replace(' ' , '\\ ') + \"txt > epubs_txt/tmp.txt\")\n",
    "    os.system(\"mv epubs_txt/tmp.txt epubs_txt/\" + file[:-4].replace(' ' , '\\ ') + 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fList = os.listdir('epubs_txt/')\n",
    "os.system('mkdir epubs_txt/splitepubs')\n",
    "for file in fList:\n",
    "    os.system('split -l 100 epubs_txt/' + file +' epubs_txt/splitepubs/split'+ file[:-4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docx\n",
    "convert random collection of .docx and .tex to txt and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fList = os.listdir('my_docs')\n",
    "for file in fList:\n",
    "    os.system('pandoc my_docs/' + file.replace(' ' , '\\ ') + ' -t plain -o my_txt/' + file[:-4].replace(' ' , '\\ ') + 'txt')\n",
    "    os.system(\"awk ' /^$/ { print; } /./ { printf(\\\"%s \\\", $0); } ' my_txt/\" + file[:-4].replace(' ' , '\\ ') + \"txt > my_txt/tmp.txt\")\n",
    "    os.system(\"mv my_txt/tmp.txt my_txt/\" + file[:-4].replace(' ' , '\\ ') + 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fList = os.listdir('my_txt/')\n",
    "os.system('mkdir my_txt/splitmytxt')\n",
    "for file in fList:\n",
    "    os.system('split -l 100 my_txt/' + file +' my_txt/splitmytxt/split'+ file[:-4]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
